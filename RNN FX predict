# learn optimization
# set alert box when program finished.

import math
import random
import numpy as np
import excelExecuterFinalV2 as ee
from sklearn.model_selection import train_test_split #数据集分割
import easygui as eg

 

random.seed()

def rand(a, b):
    return (b - a) * random.random() + a

 
def make_matrix(m, n, fill=0.0):
    mat = []
    for i in range(m):
        mat.append([fill] * n)
    return mat

 

def sigmoid(x):
    return 1.0 / (1.0 + math.exp(-x))


def sigmoid_derivative(x):
    return x * (1 - x)

 

class BPNeuralNetwork:
    def __init__(self):
        self.input_n = 0
        self.hidden_n = 0
        self.output_n = 0
        self.input_cells = []
        self.hidden_cells = []
        self.output_cells = []
        self.input_weights = []
        self.output_weights = []
        self.input_correction = []
        self.output_correction = []
        self.hidden_weights = []
        self.total_previous = 0


    #def preWeightSetting(self):


        
                   

    def setup(self, ni, nh, no):

        self.input_n = ni + 1
        self.hidden_n = nh
        self.output_n = no
        
        # init cells

        self.input_cells = [1.0] * self.input_n
        self.hidden_cells = [1.0] * self.hidden_n
        self.output_cells = [1.0] * self.output_n

        # init weights
        
        self.input_weights = make_matrix(self.input_n, self.hidden_n)
        self.output_weights = make_matrix(self.hidden_n, self.output_n)
        self.hidden_weights = make_matrix(self.hidden_n, self.hidden_n)


        # random activate
        
        for i in range(self.input_n):
            for h in range(self.hidden_n):
                self.input_weights[i][h] = rand(-0.2, 0.2)
        for h in range(self.hidden_n):
            for o in range(self.output_n):
                self.output_weights[h][o] = rand(-2.0, 2.0)
        for k in range(self.hidden_n):
            for l in range(self.hidden_n):
                self.hidden_weights[h][o] = rand(-1.0, 1.0)
        
        # init correction matrix


        #self.preWeightSetting()  #input pre-saved weight data
        self.input_correction = make_matrix(self.input_n, self.hidden_n)
        self.output_correction = make_matrix(self.hidden_n, self.output_n)
        self.hidden_correction = make_matrix(self.hidden_n, self.hidden_n)

 
    def predict(self, inputs):  #also called forward propagate

        # activate input layer

        for i in range(self.input_n - 1):
            self.input_cells[i] = inputs[i]

        # activate hidden layer  the hidden i j stand for previous link, not the current one with the next

        for j in range(self.hidden_n):
            total = 0.0
            for i in range(self.input_n):
                total += self.input_cells[i] * self.input_weights[i][j] + self.total_previous * self.hidden_weights[i][j]
            self.hidden_cells[j] = sigmoid(total)
            self.total_previous = self.hidden_cells[j]

        # activate output layer

        for k in range(self.output_n):
            total = 0.0
            for j in range(self.hidden_n):
                total += self.hidden_cells[j] * self.output_weights[j][k]
            self.output_cells[k] = sigmoid(total)
        return self.output_cells[:]   #this returns the precentage

 
    def back_propagate(self, case, label, learn, correct):

        # feed forward
        self.predict(case)

        # get output layer error
        output_deltas = [0.0] * self.output_n

        for o in range(self.output_n):
            error = label[o] - self.output_cells[o]
            output_deltas[o] = sigmoid_derivative(self.output_cells[o]) * error


        # get hidden layer error
        hidden_deltas = [0.0] * self.hidden_n

        for h in range(self.hidden_n):
            error = 0.0
            for o in range(self.output_n):
                error += output_deltas[o] * self.output_weights[h][o]
            hidden_deltas[h] = sigmoid_derivative(self.hidden_cells[h]) * error

        # update output weights
        for h in range(self.hidden_n):
            for o in range(self.output_n):
                change = output_deltas[o] * self.hidden_cells[h]
                self.output_weights[h][o] += learn * change + correct * self.output_correction[h][o]
                self.output_correction[h][o] = change


        # update input weights
        for i in range(self.input_n):
            for h in range(self.hidden_n):
                change = hidden_deltas[h] * self.input_cells[i]
                self.input_weights[i][h] += learn * change + correct * self.input_correction[i][h]
                self.input_correction[i][h] = change


        # update hidden weights
        for i in range(self.hidden_n):
            for h in range(self.hidden_n):
                change1 = hidden_deltas[h] * self.hidden_cells[i]
                self.hidden_weights[i][h] += learn * change1 + correct * self.hidden_correction[i][h]
                self.hidden_correction[i][h] = change1
                

        # get global error
        error = 0.0
        for o in range(len(label)):
            error += 0.5 * (label[o] - self.output_cells[o]) ** 2
        return error


    def gw(self): #short for getWeights

        # Print output weights
    
        for h in range(self.hidden_n):
            for o in range(self.output_n):
                print("hidden",h,"output",o,"对应的weight是",self.output_weights[h][o])
                
        # Print input weights

        for i in range(self.input_n):
            for h in range(self.hidden_n):
                print("input",i,"hidden",h,"对应的weight是",self.input_weights[i][h])

        # Print hidden weights
        
        for i in range(self.input_n):
            for h in range(self.hidden_n):
                print("hidden",i,"hidden",h,"对应的weight是",self.hidden_weights[i][h])

 

    def train(self, cases, labels, limit=10000, learn=0.02, correct=0.02): #it's default number for them, but you can always reset them

        learn1 = learn  # give learn a ratio goes with error  part1

        for j in range(limit):
            #learn = 0.9995 * learn #give learn a fixed discrease ratio
            error = 0.0
            for i in range(len(cases)):
                label = labels[i]
                case = cases[i]
                error += self.back_propagate(case, label, learn1, correct)

    
            X_train,X_test,y_train,y_test=train_test_split(cases,labels)
            sum = 0
            if j%500==0:
                predictions=[]
                for k in range(len(X_test)):
                    out=self.predict(X_test[k])#用验证集去测试
                    predictions.append(np.argmax(out))#返回预测结果
                    sum += abs((y_test[k][0]-out[0])+(y_test[k][1]-out[1]))
                accuracy = sum/len(X_test)

                #accuracy=np.mean(np.equal(predictions,y_test))#求平均值

                print('epoch:',j,'Diff:',accuracy)
                print('learn =', learn1)
                print('---------------------------------I am a robot-----------------------------------')

            learn1 = accuracy * learn   # give learn a ratio goes with error   part2

            if j%500 == 0:
                self.gw()
               

    def test(self):
        case = ee.ee('1').ds()
        label = ee.ee('2').ds()

        self.setup(10, 20, 3)
        self.train(case, label, 5000, 0.35, 0.18)
        print(self.predict([0,0,1,0.41,0.48,0.59,0.59,0.48,0.81,0.67]))
        eg.msgbox("Hey, Susu has a solution now!")


if __name__ == '__main__':
    nn = BPNeuralNetwork()
    nn.test()
